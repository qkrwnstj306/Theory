* 인사말 
#1
Good morning. My name is Junseo Park. I am a student from the department of Artificial Intelligence at Dongguk University.
I am truly honored to stand before you today at this conference to present my research findings.

안녕하세요. 제 이름은 박준서입니다. 저는 동국대학교 인공지능학과 학생입니다. 오늘 이 학회에서 제 연구 결과물을 발표하게 돼서 영광입니다.

* 개요
#2
In this presentation, I'm going to talk about style training in a broader context.

이 발표에서, 저는 넓은 의미의 스타일 학습에 대해 말하고자 합니다.

* 목차 설명
#3
Here are the contents of this presentation
i will start with introduction, DreamBooth, StyleBoost and experiment.
At the end, i will finish up with conclusion

목차입니다. introduction -> DreamBooth -> StyleBoost -> experiment -> conclusion 순으로 진행하겠습니다.

* intro
#4
Recently, there has been significant attention on text-to-image models like Stable Diffusion. 

최근 stable diffusion과 같은 text로 image를 생성하는 model이 주목을 받고있습니다. 

#5
Furthermore, techniques for personalizing text-to-image models, such as Dreambooth and Textual Inversion, have also gained prominence.

더불어서, DreamBoot나 textual inversion과 같이, text로 image를 생성하는 model을 개인화시키는 기술 또한 각광을 받고 있습니다.

* DreamBooth를 설명하는 이유
#6 
Since we used DreamBooth's framework, i will explain our method by comparing it with DreamBooth.

우리는 DreamBooth의 framework를 사용하기 때문에 DreamBooth와 비교해서 설명드리겠습니다.

* Dreambooth
#7
Dreambooth has the ability to connect specific objects to unique tokens using three to five pictures through overall model fine-tuning.

DreamBooth는 모델 전체의 미세조정을 통하여 3~5장의 이미지를 사용해, 특정 object를 unique token에 연결하는 능력을 가지고 있습니다.

The process is as follows

프로세스는 다음과 같습니다.

model learns to map specific objects to unique tokens.

model은 특정 object와 unique token을 mapping하는 걸 학습합니다.

Afterwards, if you enter a prompt containing a unique token, you will be able to create an image reflecting a specific object.

그 후, unique token을 포함한 prompt를 입력하면, 특정 object를 반영한 이미지를 생성할 수 있게됩니다.


#8
However, since learning is done with few images, several problems arise.
Typical examples are overfitting and language drift.
하지만, 적은 이미지로 학습하기 때문에 여러 문제가 발생합니다.
일반적인 예로는 과적합과 언어 드리프트가 있습니다.

#9
When overfitting occurs, the model generates objects or backgrounds contained in 3 to 5 specific images, regardless of the prompt. Additionally, text-image alignment capabilities are reduced.

overfitting이 발생하면, model은 prompt와 상관없이 특정 3~5장의 object나 배경을 생성합니다. 또한, text-image alignment 능력이 감소합니다.

#10
When language drift occurs, the various meanings of a class are lost.

language drift가 발생하면, class가 가지는 다양한 의미를 상실합니다.

#11
DreamBooth introduced a class prior image to solve language drift and overfitting.

DreamBooth는 language drift와 overfiting을 해결하기 위해 class prior image라는 것을 도입했습니다.

The class prior image is mapped to the class prompt to preserve the diversity of the class. At this time, the class prior image is created through a pre-trained diffusion model.

class prior image는 class가 가지는 다양성을 보존하기 위해, class prompt에 mapping 됩니다. 이때, class prior image는 pre-trained diffusion model을 통해서 생성됩니다.

#12 
This is DreamBooth’s loss function. In the first term, the images generated by the diffusion model learn to create a specific image. In the second term, the images generated by the diffusion model learn to create various class images

DreamBooth의 loss fucntion입니다. 첫 번째 텀은 diffusion model이 생성한 이미지가 특정 이미지를 생성하게끔 학습을 합니다. 두 번째 텀은 diffusion model이 생성한 이미지가 다양한 class image를 생성하게끔 학습합니다. 	


* 우리의 목적
#13
This is the explanation of dreambooth. From now on, I will talk about style training.

여기까지가 dreambooth에 대한 설명이었습니다. 지금부턴 style training에 대해 말씀드려보겠습니다.

#14
first, We studied using an existing dreambooth and conducted experiments on three common styles.
These styles are called target style we want to learn

먼저, 기존의 dreambooth를 사용하여 학습을 해봤습니다.
3가지의 흔한 style에 대해서 실험을 진행했습니다.
우리가 학습하고자 하는 style을 target style이라고 부릅니다.

#15
This is the result.  Images generated by inputting a prompt called girl and a prompt called panda reflecting a specific style.
The pictures I'm showing you now are images generated by the method proposed in this paper. You can see that ours created reliable images by better reflecting a particular style. Before I explain how we approached it, let me first tell you why the existing DreamBooth did not work.

그 결과입니다. 특정 style을 반영한 girl이라는 prompt와 panda라는 prompt를 입력으로 생성한 이미지입니다. 
지금 보여드리는 그림들은 본 논문에서 제안한 방법으로 생성한 이미지입니다. 특정 style을 더 잘 반영하여 신뢰성 있는 이미지를 생성한 것을 볼 수 있습니다. 어떤 방식으로 접근했는지 설명을 드리기전에, 저희가 생각한 기존의 dreambooth가 왜 안됐는지를 말씀드리겠습니다.

#16
First, style is an abstract and broad concept. Therefore, it is very difficult to bind a style with only three to five images.

먼저, style은 추상적이고 넓은 개념입니다. 따라서 기존의 3~5장의 이미지만으로 style을 binding 시키는 것은 매우 어렵습니다. 

#17
The second is that the class prior image suggested by dreambooth is not helpful in style training. The class prior image created by style token was not an art style, and the class prior image created by illustration style token was inconsistent.

두 번째는, dreambooth에서 제안하는 class prior image가 style traning에서는 별 도움이 안된다는 것 입니다. style token으로 생성한 class prior image는 art style이 아니고, illustration style token으로 생성한 class prior image는 일관성이 없었습니다. 

#18
We wanted to solve additional problems that arise when we focus on style training.

우리는 style training으로 초점을 맞췄을때, 추가적으로 발생하는 문제들을 해결하고자 했습니다.

#19
There are two major changes. First, we increased the number of images to 20. We call this the style reference image.
The second is an extension of the class prior image to suit the purpose of style training. We call these images auxiliary image.

변경한 것은 크게 2가지 입니다. 첫 번째로는 이미지의 수를 20장으로 늘렸습니다. 우리는 이걸 style reference image라고 부릅니다.
두 번쨰는, class prior image를 style training 목적에 맞게 확장시킨 것 입니다. 우리는 이 이미지들을 auxiliary image라고 부릅니다.

#20
We empirically found that 20 style reference images perform well while maintaining the concept of learning with fewer images.

우리는 경험적으로 20장의 style reference image가 적은 이미지로 학습하는 컨셉을 유지하면서 좋은 성능을 낸다는 것을 알았습니다.

#21
In addition, it is difficult to bind abstract concepts only with style reference images, so we use the auxiliary image, which is similar to the target style and has more general characteristics, to help bind.

또한, style reference image만으로 추상적인 개념을 binding하는 것이 어려우므로 target style과 비슷하면서 더 일반적인 특성을 가지고 있는 auxiliary image를 binding을 돕는 용도로 사용합니다. 

#22 
Unlike object, style has a wide area,
If any image is provided within the style category, language drift will not occur. In this paper, the meaning of the style token was moved from fashion style to art style within the style category 

object와는 달리 style은 넓은 영역을 가지고 있기 때문에, 
어떤 이미지든 style 카테고리안에서 제공한다면, language drift가 발생하지 않습니다. 본 논문에서는 fashion style에서 art style로 style 카테고리 안에서 style token이 가지는 의미를 옮겼습니다.

#23
We don't have to consider language drfit, so we can configure auxiliary images with good quality on the Internet.

language drfit를 고려하지 않아도 돼서, 인터넷에서 좋은 퀄리티로 auxiliary image를 구성할 수 있게 됩니다.

#24
I'm going to show you the results of the experiment again with the methods that I just described.
We used stable diffusion 1.5 and experimented with the same 3 styles.
FID and CLIP score were used as evaluation criteria.

방금 설명드린 방법들로 다시 실험한 결과를 보여드리겠습니다.
stable diffusion 1.5를 사용했고 동일하게 3 가지 스타일에 대해서 실험했습니다.
평가 기준으로는 FID와 CLIP score를 사용했습니다.

#25
The FID score is a measure of statistical similarity. The lower the better.

FID는 확률적인 유사도를 측정한 점수입니다. 낮을수록 좋습니다.

The CLIP score is a measure of text-image alignment. The higher the better.

CLIP score는 text-image alignment를 측정한 점수입니다. 높을수록 좋습니다.

#26
We made two assumptions before the experiment.
One is that optimal learning section is different for each style
The other is that SD 1.5 is close to realism because it was learned with a realistic image.

우리는 실험하기 전에, 2가지 가정을 했습니다. 
style마다 최적의 학습 구간이 다르다는 것과 SD 1.5는 realistic image로 학습을 했기 때문에 realism과 가깝다는 것 입니다.

#27
The first experiment is about the traninig step.
The first message from this experiment is that optimal training section for all styles occurs in fewer training steps. Accordingly, text image alignment is preserved.

첫 번째 실험은 traninig step에 대한 실험입니다. 
이 실험이 전하는 첫 번째 메세지는 모든 style에 대한 학습이 적은 training step으로 이루어진다는 것 입니다. 그에 따라 text image alignment가 보존이 됩니다.

#28
The second message is that, as initially assumed, the further a style is from realism, the more training is required.

두 번째 메세지는 처음 가정한대로, realism에서 먼 스타일일수록 더 많은 학습이 필요하다는 것입니다.

#29
The second experiment is about configuring a style reference image.
If you configure a style reference image with only a background, you can see that the background is created from the prompt Girl, and if you configure a style reference image with only people, you can see that water is created from the prompt water. In other words, if it consists of only one thing, it has biased information. In contrast, a combination of the two creates images reflecting the prompt.

두 번째 실험은 style reference image 구성에 대한 실험입니다. 
background로만 style reference image를 구성하면 girl이라는 prompt에 배경을 생성하는 걸 볼 수 있고, 사람으로만 style reference image를 구성하면 water이라는 prompt에 사람을 생성하는 걸 볼 수 있습니다. 즉, 한 가지로만 구성하면 biased information을 가집니다. 그에 반해, 둘을 섞은 구성은 prompt를 반영하면서 이미지를 생성합니다. 

#30
This is the result of measuring the FID score for each configuration. Because style consists of multiple elements, composing images with only one element can result in learning biased information.

각 구성에 대해서 FID score를 측정한 결과입니다. style은 여러 요소로 이루어져 있기 때문에 한 가지로만 image를 구성하면 편향된 정보를 학습할 수 있습니다. 

#31
The last experiment is about the auxiliary image.
First, you need to know the definition of the digital painting image.
A digital painting image is an image implemented by a software tool. 	
The figures below are example images of digital painting.

마지막 실험은 auxiliary image에 대한 것입니다. 
먼저 digital painting image에 대한 정의를 알아야합니다. 
digital painting image는 software tool로 구현한 이미지입니다. 아래 그림은 digital painting의 예시 이미지입니다.

#32
We used the digital painting image as the auxiliary image.
This is because the target style dataset was created with a software tool called diffusion model. Therefore, the digital painting image satisfies the figure below.

저희는 digital painting image를 auxiliary image로 사용했습니다.
target style dataset을 diffusion model이라는 software tool로 생성했기 때문입니다. 따라서 digital painting image는 아래의 그림을 만족합니다.

#33
This table shows that the performance increases because the auxiliary image helps bind.

이 table은 auxiliary image가 binding을 도와주기 때문에, 성능이 증가함을 보여줍니다. 

#34
The result shown previously was generated through these experiments.
You can confirm that performance is improved not only through qualitative evaluation but also through FID score.

이전에 보여드린 결과 이미지는 바로 이런 방식을 통해 생성했습니다.
정성적인 평가뿐 아니라 FID score로도 성능이 향상됨을 확인할 수 있습니다.

#35
Here's the conclusion.
From the experimental results, we can see that training is still possible with fewer images. Additionally, a small training step can alleviate overfitting and preserve text-image alignment.

결론입니다. 
실험 결과로부터, 우리는 여전히 적은 이미지로 학습을 하는 것을 알 수 있습니다. 또한, 적은 training step으로 인해 overfitting 완화 효과와 text-image alignment를 보존할 수 있습니다. 

Ultimately, The core idea is to transform DreamBooth's class prior image into auxiliary image, providing a secondary binding role to bind the abstract concept.

궁극적으로, 본 논문의 core idea는 추상적인 개념을 binding 시키기 위해, class prior image가 아닌 auxiliary image로 제공하는 것에 있습니다. 

#36
Through our method, we can give you a style guide with even a small amount of images.

우리의 기법을 통해, 여러분에게 적은 이미지로도 style guide를 제공해줄 수 있습니다. 

Experimenting with more diverse styles and comparing with more diverse techniques are things that need to be improved upon in the future.

더 다양한 style로 실험하고, 더 다양한 기법들과 비교하는 것이 추후 보완해야될 점입니다. 

#37
That's it. Thank you.
Do you have any questions?

이상입니다. 감사합니다. 
질문이 있을까요?

If you don't have any questions, I'll finish the presentation. Thank you again.

질문이 없으시면, 발표를 마치겠습니다. 다시 한 번 감사합니다. 


#####target style을 설명
